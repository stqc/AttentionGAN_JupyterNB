{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f \n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import itertools\n",
    "import glob\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.autograd import Variable\n",
    "import datetime\n",
    "import time\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet blocks as per the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(resnet,self).__init__()\n",
    "        self.resblock =  self.make_block()\n",
    "    def make_block(self):\n",
    "        self.conv_block = []\n",
    "        self.conv_block+=[nn.ReflectionPad2d(1)]\n",
    "        self.conv_block+=[nn.Conv2d(256,256,kernel_size=3,padding=0,bias=True)]\n",
    "        self.conv_block+=[nn.ReflectionPad2d(1)]\n",
    "        self.conv_block+=[nn.Conv2d(256,256,kernel_size=3,padding=0,bias=True)]\n",
    "        \n",
    "        return nn.Sequential(*self.conv_block)\n",
    "    def forward(self,x):\n",
    "        return x+self.resblock(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        \n",
    "        #down sampling\n",
    "        self.conv = [nn.ReflectionPad2d(3),nn.Conv2d(3,64,kernel_size = 7,stride=1, padding = 0, bias = True),\n",
    "                      nn.InstanceNorm2d(64),\n",
    "                     nn.ReLU(True)\n",
    "                    ]\n",
    "        self.conv+= [nn.Conv2d(64,128,kernel_size=3,stride=2,padding=1,bias=True),\n",
    "                    nn.InstanceNorm2d(128),\n",
    "                    nn.ReLU(True)]\n",
    "        self.conv+=[nn.Conv2d(128,256,kernel_size=3,stride=2,padding=1,bias=True),\n",
    "                   nn.InstanceNorm2d(256),\n",
    "                   nn.ReLU(True)]\n",
    "        \n",
    "        #resnet layers\n",
    "        for i in range(6):\n",
    "            self.conv+=[resnet()]\n",
    "        \n",
    "        self.conv = nn.Sequential(*self.conv)\n",
    "        \n",
    "        #content mask upsampling\n",
    "        self.content_deconv = [nn.Upsample(scale_factor=2),\n",
    "                              nn.ReflectionPad2d(1),\n",
    "                              nn.Conv2d(256,128,kernel_size=3,stride=1,padding=0,bias=True),\n",
    "                              nn.InstanceNorm2d(128),\n",
    "                              nn.ReLU(True)]\n",
    "        self.content_deconv += [nn.Upsample(scale_factor=2),\n",
    "                              nn.ReflectionPad2d(1),\n",
    "                              nn.Conv2d(128,64,kernel_size=3,stride=1,padding=0,bias=True),\n",
    "                              nn.InstanceNorm2d(64),\n",
    "                              nn.ReLU(True)]\n",
    "        self.content_deconv+= [nn.ReflectionPad2d(3),\n",
    "                              nn.Conv2d(64,30,kernel_size=7,stride=1,padding=0,bias=True),\n",
    "                              ]\n",
    "        self.content_deconv = nn.Sequential(*self.content_deconv)\n",
    "        #attention mask upsampling\n",
    "        \n",
    "        self.attention_deconv = [nn.Upsample(scale_factor=2),\n",
    "                              nn.ReflectionPad2d(1),\n",
    "                              nn.Conv2d(256,128,kernel_size=3,stride=1,padding=0,bias=True),\n",
    "                              nn.InstanceNorm2d(128),\n",
    "                              nn.ReLU(True)]\n",
    "        self.attention_deconv+= [nn.Upsample(scale_factor=2),\n",
    "                              nn.ReflectionPad2d(1),\n",
    "                              nn.Conv2d(128,64,kernel_size=3,stride=1,padding=0,bias=True),\n",
    "                              nn.InstanceNorm2d(64),\n",
    "                              nn.ReLU(True)]\n",
    "        self.attention_deconv+= [nn.ReflectionPad2d(3),\n",
    "                              nn.Conv2d(64,10,kernel_size=7,stride=1,padding=0,bias=True),\n",
    "                              ]\n",
    "        self.attention_deconv = nn.Sequential(*self.attention_deconv)\n",
    "        self.tanh=  nn.Tanh()\n",
    "        \n",
    "    def forward(self,x):\n",
    "       \n",
    "        x = self.conv(x)\n",
    "        content = self.content_deconv(x)\n",
    "        attention = self.attention_deconv(x)\n",
    "        \n",
    "        image = self.tanh(content)\n",
    "        \n",
    "        image1 = image[:, 0:3, :, :]\n",
    "        image2 = image[:, 3:6, :, :]\n",
    "        image3 = image[:, 6:9, :, :]\n",
    "        image4 = image[:, 9:12, :, :]\n",
    "        image5 = image[:, 12:15, :, :]\n",
    "        image6 = image[:, 15:18, :, :]\n",
    "        image7 = image[:, 18:21, :, :]\n",
    "        image8 = image[:, 21:24, :, :]\n",
    "        image9 = image[:, 24:27, :, :]\n",
    "        image10 = image[:, 27:30, :, :]\n",
    "        \n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        attention = softmax(attention)\n",
    "        \n",
    "        attention1_ = attention[:, 0:1, :, :]\n",
    "        attention2_ = attention[:, 1:2, :, :]\n",
    "        attention3_ = attention[:, 2:3, :, :]\n",
    "        attention4_ = attention[:, 3:4, :, :]\n",
    "        attention5_ = attention[:, 4:5, :, :]\n",
    "        attention6_ = attention[:, 5:6, :, :]\n",
    "        attention7_ = attention[:, 6:7, :, :]\n",
    "        attention8_ = attention[:, 7:8, :, :]\n",
    "        attention9_ = attention[:, 8:9, :, :]\n",
    "        attention10_ = attention[:, 9:10, :, :]\n",
    "\n",
    "        attention1_ = attention1_.repeat(1, 3, 1, 1)\n",
    "        attention2_ = attention2_.repeat(1, 3, 1, 1)\n",
    "        attention3_ = attention3_.repeat(1, 3, 1, 1)\n",
    "        attention4_ = attention4_.repeat(1, 3, 1, 1)\n",
    "        attention5_ = attention5_.repeat(1, 3, 1, 1)\n",
    "        attention6_ = attention6_.repeat(1, 3, 1, 1)\n",
    "        attention7_ = attention7_.repeat(1, 3, 1, 1)\n",
    "        attention8_ = attention8_.repeat(1, 3, 1, 1)\n",
    "        attention9_ = attention9_.repeat(1, 3, 1, 1)\n",
    "        attention10_ = attention10_.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        op1 = image1*attention1_\n",
    "        op2 = image2*attention2_\n",
    "        op3 = image3*attention3_\n",
    "        op4 = image4*attention4_\n",
    "        op5 = image5*attention5_\n",
    "        op6 = image6*attention6_\n",
    "        op7 = image7*attention7_\n",
    "        op8 = image8*attention8_\n",
    "        op9 = image9*attention9_\n",
    "        op10 = image10*attention10_\n",
    "        \n",
    "        op = op1+op2+op3+op4+op5+op6+op7+op8+op9+op10\n",
    "        \n",
    "        return op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        model=[nn.Conv2d(3,64,kernel_size=4,stride=2,padding=1,bias=True),\n",
    "              nn.LeakyReLU(0.02,True)]\n",
    "        \n",
    "        model +=[nn.Conv2d(64,128,kernel_size=4,stride=2,padding=1,bias=True),\n",
    "                nn.InstanceNorm2d(128),\n",
    "                nn.LeakyReLU(0.02,True)]\n",
    "        \n",
    "        model +=[ nn.Conv2d(128,256,kernel_size=4,stride=2,padding=1,bias=True),\n",
    "                nn.InstanceNorm2d(128),\n",
    "                nn.LeakyReLU(0.02,True)]\n",
    "       \n",
    "        model +=[ nn.Conv2d(256,512,kernel_size=4,stride=2,padding=1,bias=True),\n",
    "                nn.InstanceNorm2d(512),\n",
    "                nn.LeakyReLU(0.02,True)]\n",
    "        \n",
    "        model +=[nn.ZeroPad2d((1,0,1,0))]\n",
    "        \n",
    "        model +=[nn.Conv2d(512,1,kernel_size=4,stride=1,padding=1,bias=True)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",     
    "        \n",
    "    def  forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv')!=-1:\n",
    "        torch.nn.init.normal_(m.weight.data,0.0,0.02)\n",
    "    elif classname.find('BatchNorm2d')!=-1:\n",
    "        torch.nn.init.normal_(m.weight.data,1.0,0.02)\n",
    "        torch.nn.init.constant_(m.bias.data,0.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buffer to store previously generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class replaybuffer():\n",
    "    def __init__(self,max_size=50):\n",
    "        self.max_size= max_size\n",
    "        self.data =[]\n",
    "    \n",
    "    def push_and_pop(self,data):\n",
    "        to_return =[]\n",
    "        for element in data.data:\n",
    "            element= torch.unsqueeze(element,0)\n",
    "            if len(self.data)<self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if np.random.uniform(0,1)>0.5:\n",
    "                    i = np.random.randint(0,self.max_size-1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.autograd.Variable(torch.cat(to_return))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class to reducing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        \n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_A = Generator()\n",
    "generator_B = Generator()\n",
    "discriminator_A = Discriminator()\n",
    "discriminator_B = Discriminator()\n",
    "generator_A.apply(weights_init)\n",
    "generator_B.apply(weights_init)\n",
    "discriminator_A.apply(weights_init)\n",
    "discriminator_B.apply(weights_init)\n",
    "# ignore this if not running on GPU\n",
    "generator_A.cuda()\n",
    "generator_B.cuda()\n",
    "discriminator_A.cuda()\n",
    "discriminator_B.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerG = torch.optim.Adam(itertools.chain(generator_A.parameters(),generator_B.parameters()),lr = 0.0002, betas=(0.5,0.999),weight_decay=1e-5)\n",
    "optimizerDA = torch.optim.Adam(discriminator_A.parameters(),lr = 0.0002, betas=(0.5,0.999))\n",
    "optimizerDB = torch.optim.Adam(discriminator_B.parameters(),lr=0.0002,betas =(0.5,0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GANloss = nn.MSELoss()\n",
    "cycle = nn.L1Loss()\n",
    "identity = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_g = torch.optim.lr_scheduler.LambdaLR(optimizerG,lr_lambda=LambdaLR(200,0,100).step)\n",
    "lr_da = torch.optim.lr_scheduler.LambdaLR(optimizerDA,lr_lambda=LambdaLR(200,0,100).step)\n",
    "lr_db = torch.optim.lr_scheduler.LambdaLR(optimizerDB,lr_lambda=LambdaLR(200,0,100).step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_A_buffer = replaybuffer()\n",
    "fake_B_buffer = replaybuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, unaligned=False, mode='train'):\n",
    "        self.transform = transforms_\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, '%s/a' % mode) + '/*.*'))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, '%s/b' % mode) + '/*.*'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
    "\n",
    "        if self.unaligned:\n",
    "            item_B = self.transform(Image.open(self.files_B[np.random.randint(0, len(self.files_B) - 1)]))\n",
    "        else:\n",
    "            item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n",
    "\n",
    "        return {'A': item_A, 'B': item_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm=torchvision.transforms.Compose(\n",
    "[torchvision.transforms.Resize(128),\n",
    " torchvision.transforms.ToTensor(),\n",
    " torchvision.transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "        \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ImageDataset('/home/prateek/Desktop/data1/', transforms_=tm, unaligned=True), \n",
    "                        batch_size=1, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch size, channels, row(depens on image size), columns(depens on image size)\n",
    "target_real = torch.tensor(np.ones((1,1,8,8)),dtype = torch.float32).cuda() \n",
    "target_fake = torch.tensor(np.zeros((1,1,8,8)),dtype= torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "### AttentionGAN converges really fast as compared to CycleGAN thus, the number of epochs I used was 60 which was also recommended in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training loop\n",
    "for epoch in range(0,60):\n",
    "    for i,batch in enumerate(dataloader):\n",
    "        generator_A.train()\n",
    "        generator_B.train()\n",
    "        optimizerG.zero_grad()\n",
    "       # optimizerD.requires_grad = False\n",
    "        setA = batch['A'].cuda()\n",
    "        setB = batch['B'].cuda()\n",
    "        \n",
    "        fakeA = generator_A(setB)\n",
    "        fakeB = generator_B(setA)\n",
    "        \n",
    "        rec_A = generator_A(fakeB)\n",
    "        rec_B = generator_B(fakeA)\n",
    "        \n",
    "        pred_fake_A = discriminator_A(fakeA)\n",
    "        gla = GANloss(pred_fake_A,target_real)\n",
    "        \n",
    "        pred_fake_B = discriminator_B(fakeB)\n",
    "        glb = GANloss(pred_fake_B,target_real)\n",
    "        \n",
    "        cycle_a = cycle(rec_A,setA)\n",
    "        cycle_b = cycle(rec_B,setB)\n",
    "        \n",
    "        identity_A = generator_A(setA)\n",
    "        identity_B = generator_B(setB)\n",
    "        \n",
    "        identity_loss =(identity(identity_A,setA)+identity(identity_B,setB))/2\n",
    "        \n",
    "        generator_loss = (gla+glb)/2+((cycle_a+cycle_b)/2)*10.0+identity_loss*5.0\n",
    "        \n",
    "        generator_loss.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        #########################################\n",
    "        optimizerDA.requires_grad = True\n",
    "        optimizerDA.zero_grad()\n",
    "        \n",
    "        pred_real_A = discriminator_A(setA)\n",
    "        dar_loss= GANloss(pred_real_A,target_real)\n",
    "        \n",
    "        pred_fake_A = discriminator_A(fake_A_buffer.push_and_pop(fakeA))\n",
    "        daf_loss = GANloss(pred_fake_A,target_fake)\n",
    "        \n",
    "        da_loss = (dar_loss+daf_loss)/2\n",
    "        da_loss.backward()\n",
    "        optimizerDA.step()\n",
    "        \n",
    "        ###########################################\n",
    "        optimizerDB.requires_grad=True\n",
    "        optimizerDB.zero_grad()\n",
    "        pred_real_B = discriminator_B(setB.detach())\n",
    "        dbr_loss = GANloss(pred_real_B,target_real)\n",
    "        \n",
    "        pred_fake_b = discriminator_B(fake_B_buffer.push_and_pop(fakeB))\n",
    "        dbf_loss = GANloss(pred_fake_b,target_fake)\n",
    "        \n",
    "        db_loss = (dbf_loss+dbr_loss)/2\n",
    "        db_loss.backward()\n",
    "        optimizerDB.step()\n",
    "        \n",
    "        ###########################################\n",
    "        \n",
    "        if(i%100==0 or i==0):\n",
    "            fakeim = torch.cat([setB,fakeA,setA,fakeB],2)\n",
    "            fakeim = fakeim.squeeze(0)\n",
    "            fakeim = fakeim*.5+.5\n",
    "            fakeim = np.transpose(fakeim.cpu().detach())\n",
    "            plt.imshow(fakeim)\n",
    "            plt.show()\n",
    "\n",
    "            print(f'epoch: {epoch} G:{generator_loss}, D:{db_loss+da_loss}, Cycle:{((cycle_a+cycle_b)/2)*10}')\n",
    "    lr_da.step()\n",
    "    lr_db.step()\n",
    "    lr_g.step()\n",
    "    \n",
    "    torch.save(generator_A.state_dict(),'/home/prateek/Desktop/GA.pt')\n",
    "    torch.save(generator_B.state_dict(),'/home/prateek/Desktop/GB.pt')\n",
    "    torch.save(discriminator_A.state_dict(),'/home/prateek/Desktop/DA.pt')\n",
    "    torch.save(discriminator_B.state_dict(),'/home/prateek/Desktop/DB.pt')\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
